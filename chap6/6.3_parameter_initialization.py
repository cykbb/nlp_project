#!/usr/bin/env python
# coding: utf-8

# In[1]:


import torch
from torch import nn


# In[2]:


net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(), nn.LazyLinear(1))
X = torch.rand(size=(2, 4))
net(X).shape


# In[3]:


def init_normal(module):
    if isinstance(module, nn.Linear):  
        nn.init.normal_(module.weight, mean=0, std=0.01)
        nn.init.zeros_(module.bias)

net.apply(init_normal)
net[0].weight.data[0], net[0].bias.data[0]


# ## Pylanceç±»åž‹æ£€æŸ¥è­¦å‘Šè§£é‡Š
# 
# ä½ çœ‹åˆ°çš„ `"__getitem__" method not defined on type "Module"` è­¦å‘Šæ˜¯Pylanceï¼ˆPythonç±»åž‹æ£€æŸ¥å™¨ï¼‰çš„ä¸€ä¸ªé™åˆ¶ã€‚
# 
# ### é—®é¢˜åŽŸå› ï¼š
# - `nn.Sequential` ç»§æ‰¿è‡ª `nn.Module`
# - Pylanceåªçœ‹åˆ°é™æ€ç±»åž‹ `nn.Module`ï¼Œä¸çŸ¥é“ `nn.Sequential` å®žé™…ä¸Šå®žçŽ°äº† `__getitem__` æ–¹æ³•
# - æ‰€ä»¥å®ƒè®¤ä¸º `net[0]` æ˜¯æ— æ•ˆçš„ç´¢å¼•æ“ä½œ
# 
# ### å®žé™…æƒ…å†µï¼š
# - ä»£ç è¿è¡Œå®Œå…¨æ­£å¸¸ï¼Œå› ä¸º `nn.Sequential` ç¡®å®žæ”¯æŒç´¢å¼•
# - è¿™åªæ˜¯ä¸€ä¸ªé™æ€ç±»åž‹æ£€æŸ¥çš„å‡é˜³æ€§è­¦å‘Š

# In[4]:


# è§£å†³æ–¹æ¡ˆ1: ä½¿ç”¨ç±»åž‹æ³¨è§£æ˜Žç¡®æŒ‡å®šç±»åž‹
from typing import cast

net_typed = cast(nn.Sequential, net)
print("ä½¿ç”¨ç±»åž‹è½¬æ¢:")
print(f"ç¬¬ä¸€å±‚æƒé‡: {net_typed[0].weight.data[0]}")
print(f"ç¬¬ä¸€å±‚åç½®: {net_typed[0].bias.data[0]}")

# è§£å†³æ–¹æ¡ˆ2: ä½¿ç”¨ getattr æˆ– named_modules()
print("\nä½¿ç”¨ named_modules():")
for name, module in net.named_modules():
    if isinstance(module, nn.Linear):
        print(f"æ¨¡å— {name}: æƒé‡å½¢çŠ¶ {module.weight.data.shape}")
        print(f"æƒé‡å‰å‡ ä¸ªå€¼: {module.weight.data[0][:4]}")
        break

# è§£å†³æ–¹æ¡ˆ3: ä½¿ç”¨åˆ—è¡¨è®¿é—®ï¼ˆæŽ¨èç”¨äºŽæ•™å­¦ï¼‰
print("\nä½¿ç”¨ list() è½¬æ¢:")
layers = list(net.children())
first_layer = layers[0]
print(f"ç¬¬ä¸€å±‚ç±»åž‹: {type(first_layer)}")
if isinstance(first_layer, nn.Linear):
    print(f"æƒé‡: {first_layer.weight.data[0][:4]}")
    print(f"åç½®: {first_layer.bias.data[0]}")


# In[5]:


# è§£å†³æ–¹æ¡ˆ4: æœ€ç®€å•çš„æ–¹å¼ - æ·»åŠ ç±»åž‹å¿½ç•¥æ³¨é‡Š
print("æœ€ç®€å•çš„è§£å†³æ–¹æ¡ˆ - ä½¿ç”¨ç±»åž‹å¿½ç•¥:")
print(f"æƒé‡: {net[0].weight.data[0][:4]}")  # type: ignore
print(f"åç½®: {net[0].bias.data[0]}")        # type: ignore

# è§£å†³æ–¹æ¡ˆ5: æ›´å¥½çš„ç±»åž‹å®‰å…¨å†™æ³•ï¼ˆæŽ¨èç”¨äºŽç”Ÿäº§ä»£ç ï¼‰
def get_layer_params(sequential_net: nn.Sequential, layer_index: int):
    """å®‰å…¨åœ°èŽ·å–Sequentialç½‘ç»œä¸­æŒ‡å®šå±‚çš„å‚æ•°"""
    layers = list(sequential_net.children())
    if layer_index >= len(layers):
        raise IndexError(f"Layer index {layer_index} out of range")
    
    layer = layers[layer_index]
    if isinstance(layer, nn.Linear):
        return layer.weight.data, layer.bias.data
    else:
        raise TypeError(f"Layer {layer_index} is not a Linear layer")

print("\nç±»åž‹å®‰å…¨çš„è®¿é—®:")
weight, bias = get_layer_params(net, 0)
print(f"æƒé‡å‰å‡ ä¸ªå€¼: {weight[0][:4]}")
print(f"åç½®: {bias[0]}")


# ## æ€»ç»“ï¼šå¦‚ä½•å¤„ç†è¿™ä¸ªPylanceè­¦å‘Š
# 
# ### ðŸŽ¯ **æŽ¨èè§£å†³æ–¹æ¡ˆ**ï¼ˆæŒ‰ä½¿ç”¨åœºæ™¯ï¼‰ï¼š
# 
# 1. **ðŸ“š å­¦ä¹ /æ•™å­¦ä»£ç **ï¼šä½¿ç”¨ `# type: ignore` æ³¨é‡Š
#    ```python
#    net[0].weight.data[0]  # type: ignore
#    ```
# 
# 2. **ðŸ”§ ç”Ÿäº§ä»£ç **ï¼šä½¿ç”¨ç±»åž‹å®‰å…¨çš„è®¿é—®æ–¹æ³•
#    ```python
#    layers = list(net.children())
#    first_layer = layers[0]
#    ```
# 
# 3. **ðŸ“– æ˜Žç¡®æ€§**ï¼šä½¿ç”¨ `cast()` è¿›è¡Œç±»åž‹è½¬æ¢
#    ```python
#    net_typed = cast(nn.Sequential, net)
#    ```
# 
# ### âœ… **ä¸ºä»€ä¹ˆåŽŸä»£ç æ˜¯æ­£ç¡®çš„**ï¼š
# - `nn.Sequential` ç¡®å®žå®žçŽ°äº† `__getitem__` æ–¹æ³•
# - ä»£ç åœ¨è¿è¡Œæ—¶å®Œå…¨æ­£å¸¸
# - è¿™åªæ˜¯Pylanceçš„é™æ€åˆ†æžé™åˆ¶
# 
# ### ðŸš« **ä¸éœ€è¦ä¿®æ”¹åŽŸä»£ç **ï¼š
# ä½ çš„åŽŸå§‹ä»£ç  `net[0].weight.data[0]` å®Œå…¨æ­£ç¡®ï¼Œåªéœ€è¦é€‰æ‹©åˆé€‚çš„æ–¹å¼æ¶ˆé™¤è­¦å‘Šå³å¯ã€‚

# In[6]:


def init_xavier(module):
    if isinstance(module, nn.Linear):
        nn.init.xavier_uniform_(module.weight)

def init_42(module):
    if isinstance(module, nn.Linear):
        nn.init.constant_(module.weight, 42)

net[0].apply(init_xavier)
net[2].apply(init_42)
print(net[0].weight.data[0]) # type: ignore
print(net[2].weight.data)


# In[7]:


def my_init(module):
    if isinstance(module, nn.Linear):
        print("Init", [(name, param.shape)
                        for name, param in module.named_parameters()][0])
        nn.init.uniform_(module.weight, -10, 10)
        module.weight.data *= module.weight.data.abs() >= 5

net.apply(my_init)
net[0].weight[:2] # type: ignore


# In[8]:


net[0].weight.data[:] += 1 # type: ignore
net[0].weight.data[0, 0] = 42 # type: ignore
net[0].weight.data[0] # type: ignore

