import torch
import numpy as np
from typing import Generator, List, Any
from matplotlib import pyplot as plt
from d2l.plot import plot
from abc import ABC, abstractmethod

class Model(ABC):
    def __init__(self, optimizer: Any) -> None:
        self.optimizer: Any = optimizer
        
    def predict(self, X: torch.Tensor) -> torch.Tensor:
        return self.forward(X)
    
    def test(self, test_data_loader: Any) -> Any:
        total_loss = 0.0
        total_samples = 0
        for (X, y) in test_data_loader:
            y_hat = self.forward(X)
            batch_loss = self.loss(y_hat, y)  # 这已经是平均损失
            batch_size = y.shape[0]
            total_loss += batch_loss.item() * batch_size  # 转换为总损失
            total_samples += batch_size
        return total_loss / total_samples
    
    def train_epoch(self, train_data_loader: Any) -> List[float]:
        batch_loss = []
        for (X, y) in train_data_loader:
            y_hat = self.forward(X)
            loss = self.loss(y_hat, y)
            loss.backward()
            batch_loss.append(loss.item())
            self.optimizer.step()
            self.optimizer.zero_grad()
        return batch_loss
        
    def train(self,
              train_data_loaders: Generator[Any, None, None]) -> List[List[float]]:
        all_epoch_loss = []
        for train_data_loader in train_data_loaders:
            epoch_loss = self.train_epoch(train_data_loader)
            all_epoch_loss.append(epoch_loss)
        return all_epoch_loss
    
    def plot_loss(self, all_epoch_loss: List[List[float]]) -> None:
        num_epochs = len(all_epoch_loss)
        num_batch = len(all_epoch_loss[0])
        x = np.arange(1, num_epochs + 1, 1 / num_batch)
        y = np.array([[batch_loss for batch_loss in epoch_loss] for epoch_loss in all_epoch_loss]).flatten()
        fig, ax = plt.subplots()
        plot(ax, (x, [y]), ('epoch', 'loss'), ((1, num_epochs), (0, max(y))), legend=['loss'])
        
    @abstractmethod
    def forward(self, X: torch.Tensor) -> torch.Tensor:
        pass

    @abstractmethod
    def loss(self, y_hat: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        pass
    
class ModelTorch(Model):
    def __init__(self, net: torch.nn.Module, loss_fn: Any, optimizer: Any) -> None:
        super().__init__(optimizer)
        self.net: torch.nn.Module = net
        self.loss_fn: Any = loss_fn
        
    def forward(self, X: torch.Tensor) -> torch.Tensor:
        return self.net(X)
    
    def loss(self, y_hat: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        return self.loss_fn(y_hat, y)

    def apply_init(self, inputs: torch.Tensor, init_fn: Any) -> None:
        self.net(inputs)  
        if init_fn is not None:
            self.net.apply(init_fn)
            
    def to_device(self, device: torch.device) -> None:
        """将模型和优化器状态迁移到指定设备
        
        Args:
            device: 目标设备 (如 torch.device('cuda'), torch.device('cpu'))
        """
        # 迁移网络参数
        self.net.to(device)
        
        # 迁移优化器状态 (如动量缓冲区等)
        for param_group in self.optimizer.param_groups:
            for param in param_group['params']:
                param_state = self.optimizer.state.get(param)
                if param_state is not None:
                    for key, value in param_state.items():
                        if isinstance(value, torch.Tensor):
                            param_state[key] = value.to(device)
    
    def cuda(self) -> 'ModelTorch':
        """迁移到CUDA设备 (如果可用)"""
        if torch.cuda.is_available():
            device = torch.device('cuda')
            self.to_device(device)
        else:
            print("CUDA不可用，保持在CPU上")
        return self
    
    def cpu(self) -> 'ModelTorch':
        """迁移到CPU设备"""
        device = torch.device('cpu')
        self.to_device(device)
        return self
    
    def mps(self) -> 'ModelTorch':
        """迁移到MPS设备 (Apple Silicon)"""
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            device = torch.device('mps')
            self.to_device(device)
        else:
            print("MPS不可用，保持在当前设备")
        return self